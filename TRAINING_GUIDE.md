# üöÄ TRAINING GUIDE

## üìã Prerequisites

### Y√™u c·∫ßu ph·∫ßn c·ª©ng:
- **GPU**: NVIDIA RTX 3060+ (8GB VRAM tr·ªü l√™n) - **B·∫ÆT BU·ªòC**
- **RAM**: 16GB+
- **Storage**: 50GB+ dung l∆∞·ª£ng tr·ªëng
- **CPU**: 8+ cores (khuy·∫øn ngh·ªã)

### Y√™u c·∫ßu ph·∫ßn m·ªÅm:
- Python 3.10+
- CUDA 11.8 ho·∫∑c 12.1
- Conda ho·∫∑c virtualenv

---

## üõ†Ô∏è Setup Environment

### B∆∞·ªõc 1: Clone Repository

```bash
git clone <repository-url>
cd DEEP_LEARNING
```

### B∆∞·ªõc 2: T·∫°o Conda Environment

```bash
# T·∫°o environment m·ªõi
conda create -n milk10k python=3.10
conda activate milk10k
```

### B∆∞·ªõc 3: C√†i ƒë·∫∑t PyTorch v·ªõi CUDA

**Quan tr·ªçng**: C√†i PyTorch v·ªõi CUDA support tr∆∞·ªõc khi c√†i c√°c package kh√°c!

```bash
# Ki·ªÉm tra CUDA version
nvidia-smi

# Cho CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Cho CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Ki·ªÉm tra PyTorch ƒë√£ nh·∫≠n GPU ch∆∞a
python -c "import torch; print(torch.cuda.is_available())"
# Ph·∫£i in ra: True
```

### B∆∞·ªõc 4: C√†i ƒë·∫∑t Dependencies

```bash
pip install -r requirements.txt
```

---

## üì¶ Chu·∫©n b·ªã Dataset

### Option 1: S·ª≠ d·ª•ng preprocessed data c√≥ s·∫µn

N·∫øu c√≥ member ƒë√£ ch·∫°y preprocessing, download c√°c file sau:
- `preprocessed_data/train_data.csv`
- `preprocessed_data/val_data.csv`
- `preprocessed_data/class_weights.json`

### Option 2: Ch·∫°y preprocessing t·ª´ ƒë·∫ßu

```bash
# ƒê·∫£m b·∫£o c√≥ dataset g·ªëc trong th∆∞ m·ª•c dataset/
python src/data_preprocessing.py
```

K·∫øt qu·∫£:
```
preprocessed_data/
‚îú‚îÄ‚îÄ train_data.csv          # 4,192 samples
‚îú‚îÄ‚îÄ val_data.csv            # 1,048 samples
‚îî‚îÄ‚îÄ class_weights.json      # Class weights cho Focal Loss
```

---

## üèãÔ∏è Training

### Ki·ªÉm tra c·∫•u h√¨nh tr∆∞·ªõc khi train

M·ªü file `src/config.py` v√† x√°c nh·∫≠n c√°c settings:

```python
# Model config
MODEL_CONFIG = {
    'architecture': 'efficientnet_b3',
    'pretrained': True,
    'use_metadata': True,
    'metadata_dim': 18,
    'dropout': 0.3
}

# Training config
TRAIN_CONFIG = {
    'batch_size': 16,          # Gi·∫£m xu·ªëng 8 n·∫øu OOM
    'num_epochs': 100,
    'learning_rate': 1e-4,
    'early_stopping_patience': 15,
    'mixed_precision': True    # Quan tr·ªçng: gi·∫£m VRAM usage
}
```

### Start Training

```bash
python src/train.py
```

### Gi√°m s√°t Training v·ªõi TensorBoard

M·ªü terminal m·ªõi:

```bash
conda activate milk10k
tensorboard --logdir=logs

# M·ªü browser: http://localhost:6006
```

TensorBoard s·∫Ω hi·ªÉn th·ªã:
- Training/Validation loss
- Macro F1 Score
- Micro F1 Score
- Per-class F1 scores (11 classes)
- Learning rate schedule

---

## üìä Training Progress

### Th·ªùi gian d·ª± ki·∫øn:

| GPU | Batch Size | Time per Epoch | Total Time (100 epochs) |
|-----|------------|----------------|-------------------------|
| RTX 3060 (12GB) | 16 | ~8-10 min | ~10-15 gi·ªù |
| RTX 3070 (8GB) | 8-12 | ~10-12 min | ~12-18 gi·ªù |
| RTX 3080 (10GB) | 16 | ~5-7 min | ~6-10 gi·ªù |
| RTX 3090 (24GB) | 32 | ~3-5 min | ~5-8 gi·ªù |
| RTX 4090 (24GB) | 32 | ~2-3 min | ~3-5 gi·ªù |
| A100 (40GB) | 64 | ~1-2 min | ~2-3 gi·ªù |

**L∆∞u √Ω**: Early stopping c√≥ th·ªÉ d·ª´ng training s·ªõm n·∫øu F1 kh√¥ng c·∫£i thi·ªán sau 15 epochs.

### Checkpoints ƒë∆∞·ª£c l∆∞u:

```
models/
‚îú‚îÄ‚îÄ best_model.pth              # Model t·ªët nh·∫•t (theo Macro F1)
‚îú‚îÄ‚îÄ checkpoint_epoch_5.pth      # Checkpoint m·ªói 5 epochs
‚îú‚îÄ‚îÄ checkpoint_epoch_10.pth
‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ training_history.csv        # L·ªãch s·ª≠ training
```

---

## üö® Troubleshooting

### 1. CUDA Out of Memory (OOM)

**Tri·ªáu ch·ª©ng**: `RuntimeError: CUDA out of memory`

**Gi·∫£i ph√°p**:
```python
# Trong src/config.py, gi·∫£m batch_size
TRAIN_CONFIG = {
    'batch_size': 8,  # Gi·∫£m t·ª´ 16 xu·ªëng 8
    # ...
}
```

Ho·∫∑c:
```python
# Gi·∫£m image_size
IMAGE_CONFIG = {
    'image_size': 224,  # Gi·∫£m t·ª´ 384 xu·ªëng 224
    # ...
}
```

### 2. Training qu√° ch·∫≠m

**Ki·ªÉm tra**:
- Mixed precision c√≥ ƒëang b·∫≠t kh√¥ng? `TRAIN_CONFIG['mixed_precision'] = True`
- `num_workers` c√≥ ph√π h·ª£p? Th·ª≠ `num_workers = 4` ho·∫∑c `8`
- GPU c√≥ ƒëang ch·∫°y c√°c ti·∫øn tr√¨nh kh√°c kh√¥ng? Ki·ªÉm tra `nvidia-smi`

### 3. Loss kh√¥ng gi·∫£m

**Ki·ªÉm tra**:
- Class weights c√≥ load ƒë√∫ng kh√¥ng?
- Learning rate c√≥ qu√° cao? Th·ª≠ gi·∫£m xu·ªëng `5e-5`
- D·ªØ li·ªáu c√≥ ƒë∆∞·ª£c chu·∫©n h√≥a ƒë√∫ng kh√¥ng?

### 4. Validation F1 th·∫•p

**Th·ª≠**:
- Training th√™m epochs
- Thay ƒë·ªïi augmentation
- Th·ª≠ fusion strategy kh√°c (`late` thay v√¨ `early`)
- TƒÉng dropout ƒë·ªÉ tr√°nh overfitting

---

## üíæ Sau khi Training xong

### 1. Ki·ªÉm tra k·∫øt qu·∫£

```python
import pandas as pd

# Xem training history
history = pd.read_csv('models/training_history.csv')
print(history.tail(10))

# Xem best F1 score
print(f"Best Macro F1: {history['val_f1_macro'].max():.4f}")
```

### 2. Generate Submission

```bash
# Prediction c∆° b·∫£n
python src/generate_submission.py --model_path models/best_model.pth

# V·ªõi Test Time Augmentation (khuy·∫øn ngh·ªã)
python src/generate_submission.py --model_path models/best_model.pth --use_tta
```

K·∫øt qu·∫£:
- `results/submission.csv`
- `results/submission_tta.csv`

### 3. Share Model v·ªõi Team

**Option 1: Git LFS** (n·∫øu repo h·ªó tr·ª£)
```bash
git lfs install
git lfs track "*.pth"
git add models/best_model.pth
git commit -m "Add trained model (F1: 0.XXXX)"
git push
```

**Option 2: Google Drive / Dropbox**
```bash
# Upload models/best_model.pth l√™n Drive
# Share link trong team chat
```

**Option 3: Hugging Face Hub**
```bash
pip install huggingface_hub
python scripts/upload_to_hf.py  # (t·∫°o script ri√™ng n·∫øu c·∫ßn)
```

---

## üìà Expected Results

### Target Metrics (theo literature):

- **Baseline F1 (EfficientNet-B3)**: 0.70 - 0.75
- **With metadata fusion**: 0.75 - 0.80
- **With TTA + ensemble**: 0.80 - 0.85+

### Training curve m·∫´u:

```
Epoch 1/100 - 10m 23s
  Train Loss: 0.3521
  Val Loss:   0.2987
  Val F1 (Macro): 0.6234
  Learning Rate: 0.000100

Epoch 10/100 - 9m 45s
  Train Loss: 0.1834
  Val Loss:   0.1923
  Val F1 (Macro): 0.7123
  Learning Rate: 0.000092

...

Epoch 45/100 - 9m 38s
  Train Loss: 0.0923
  Val Loss:   0.1456
  Val F1 (Macro): 0.7812
  ‚úÖ Best model saved! F1: 0.7812

Early stopping triggered at epoch 60
```

---

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ:
1. Ki·ªÉm tra l·∫°i c√°c b∆∞·ªõc trong guide n√†y
2. Search error message tr√™n Google/Stack Overflow
3. H·ªèi trong team chat
4. T·∫°o issue tr√™n GitHub repo

---

## üìù Notes cho ng∆∞·ªùi Training

### Quan tr·ªçng:
- [ ] Commit training history CSV
- [ ] Commit best model (ho·∫∑c upload Drive + share link)
- [ ] Ghi l·∫°i best F1 score trong README
- [ ] Screenshot TensorBoard curves (loss, F1)
- [ ] Note l·∫°i training time v√† GPU used

### Checkpoint:
```markdown
## Training Results

- **Date**: YYYY-MM-DD
- **GPU**: RTX 3090
- **Training Time**: 6.5 hours
- **Best Epoch**: 45
- **Best Macro F1**: 0.7812
- **Best Micro F1**: 0.8156
- **Model**: models/best_model.pth
- **Download**: [Google Drive Link]
```

---

Happy Training! üöÄ
