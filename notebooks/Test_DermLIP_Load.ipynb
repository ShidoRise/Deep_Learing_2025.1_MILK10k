{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Test DermLIP Model Loading from HuggingFace\n",
        "\n",
        "This notebook tests loading the DermLIP model from HuggingFace Hub.\n",
        "\n",
        "**Model**: `redlessone/DermLIP_PanDerm-base-w-PubMed-256`\n",
        "\n",
        "**Note**: Native OpenCLIP loading fails due to `pretrain_path` in the HF config. We use **manual weight loading** instead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q open_clip_torch safetensors huggingface_hub\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Load DermLIP Visual Encoder (Manual Method)\n",
        "\n",
        "This downloads weights from HuggingFace and loads them into a ViT-B-16 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import open_clip\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "print(f\"OpenCLIP version: {open_clip.__version__}\")\n",
        "print(\"=\"*60)\n",
        "print(\"Loading DermLIP Visual Encoder (Manual Method)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Download weights from HuggingFace\n",
        "print(\"\\n1. Downloading weights from HuggingFace...\")\n",
        "weights_path = hf_hub_download(\n",
        "    repo_id=\"redlessone/DermLIP_PanDerm-base-w-PubMed-256\",\n",
        "    filename=\"open_clip_model.safetensors\"\n",
        ")\n",
        "print(f\"   ‚úÖ Downloaded to: {weights_path}\")\n",
        "\n",
        "# Step 2: Create base ViT-B-16 model (same architecture as DermLIP)\n",
        "print(\"\\n2. Creating base ViT-B-16 model...\")\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    'ViT-B-16',\n",
        "    pretrained=None  # No pretrained weights\n",
        ")\n",
        "print(f\"   ‚úÖ Model created\")\n",
        "\n",
        "# Step 3: Load DermLIP weights\n",
        "print(\"\\n3. Loading DermLIP weights...\")\n",
        "state_dict = load_file(weights_path)\n",
        "print(f\"   Total keys in weights: {len(state_dict)}\")\n",
        "\n",
        "# Extract visual encoder weights only\n",
        "visual_state = {\n",
        "    k.replace(\"visual.\", \"\"): v \n",
        "    for k, v in state_dict.items() \n",
        "    if k.startswith(\"visual.\")\n",
        "}\n",
        "print(f\"   Visual encoder keys: {len(visual_state)}\")\n",
        "\n",
        "# Load into model\n",
        "missing, unexpected = model.visual.load_state_dict(visual_state, strict=False)\n",
        "print(f\"   Missing keys: {len(missing)}\")\n",
        "print(f\"   Unexpected keys: {len(unexpected)}\")\n",
        "\n",
        "# Step 4: Test forward pass\n",
        "print(\"\\n4. Testing forward pass...\")\n",
        "model.eval()\n",
        "dummy_img = torch.randn(1, 3, 224, 224)\n",
        "with torch.no_grad():\n",
        "    features = model.encode_image(dummy_img)\n",
        "print(f\"   ‚úÖ Image features shape: {features.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ SUCCESS: DermLIP visual encoder loaded correctly!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Verify Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify architecture matches DermLIP config\n",
        "print(\"Model Architecture Verification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Expected from DermLIP open_clip_config.json:\n",
        "# image_size: 224, layers: 12, width: 768, patch_size: 16\n",
        "expected = {\n",
        "    \"image_size\": 224,\n",
        "    \"layers\": 12, \n",
        "    \"width\": 768,\n",
        "    \"patch_size\": 16,\n",
        "    \"output_dim\": 512\n",
        "}\n",
        "\n",
        "# Check visual encoder\n",
        "visual = model.visual\n",
        "print(f\"\\n‚úÖ Visual encoder output dim: {visual.output_dim}\")\n",
        "print(f\"   (Expected: {expected['output_dim']})\")\n",
        "\n",
        "# Count transformer layers\n",
        "if hasattr(visual, 'transformer') and hasattr(visual.transformer, 'resblocks'):\n",
        "    num_layers = len(visual.transformer.resblocks)\n",
        "    print(f\"‚úÖ Transformer layers: {num_layers}\")\n",
        "    print(f\"   (Expected: {expected['layers']})\")\n",
        "\n",
        "# Check embedding dimension\n",
        "if hasattr(visual, 'conv1'):\n",
        "    embed_dim = visual.conv1.out_channels\n",
        "    print(f\"‚úÖ Embedding dimension: {embed_dim}\")\n",
        "    print(f\"   (Expected: {expected['width']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Architecture matches DermLIP visual encoder! ‚úÖ\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Test on GPU (if available)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Testing on GPU...\")\n",
        "    device = torch.device('cuda')\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Test batch inference\n",
        "    batch_size = 4\n",
        "    dummy_batch = torch.randn(batch_size, 3, 224, 224).to(device)\n",
        "    \n",
        "    with torch.no_grad(), torch.amp.autocast('cuda'):\n",
        "        features = model.encode_image(dummy_batch)\n",
        "    \n",
        "    print(f\"‚úÖ Batch inference on GPU successful!\")\n",
        "    print(f\"   Input shape: {dummy_batch.shape}\")\n",
        "    print(f\"   Output shape: {features.shape}\")\n",
        "    print(f\"   GPU memory used: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU available, skipping GPU test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "\n",
        "If all tests pass, the DermLIP visual encoder is loaded correctly and ready for PanDerm training!\n",
        "\n",
        "**Next step**: Upload updated `src/models_panderm.py` to Google Drive and run `Train_PanDerm_A100.ipynb`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
